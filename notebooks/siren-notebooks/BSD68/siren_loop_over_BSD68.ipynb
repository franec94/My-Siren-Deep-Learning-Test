{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"siren_loop_over_BSD68.ipynb","provenance":[{"file_id":"1EkWvZCwSs_XglEyzSmTBxLuPcl6SWhW0","timestamp":1601546282518}],"collapsed_sections":["Yjsx1yjd9zwR","ef9EGPkzA_IG","l6P06pMexFoK","qmMEVQfdnOK0","-un2QhR5nXmn","4sLX5vsuDX7f","UgE6UqEN-zhD","8t7yjXAbtLlF"],"toc_visible":true,"mount_file_id":"1np6YBBmnB4LA560DnuIXYfKzcAqkY7gD","authorship_tag":"ABX9TyMBkpb4LRDFqUkQ+SPgaOKp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KbBwYEm396cI"},"source":["# !pip install visdom"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"laFJz7Zu9xaI"},"source":["## Imports"]},{"cell_type":"markdown","metadata":{"id":"Yjsx1yjd9zwR"},"source":["### Standard Imports"]},{"cell_type":"code","metadata":{"id":"IHt9ZFqF9s6w"},"source":["from __future__ import print_function\n","from __future__ import division\n","\n","from pprint import pprint\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as sts\n","\n","import argparse\n","import copy\n","import datetime\n","import os\n","import random\n","import skimage\n","import shutil\n","import time\n","# import visdom\n","import warnings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j62f3cAY-B0E"},"source":["### Torch imports"]},{"cell_type":"code","metadata":{"id":"dIlC57oD-EQ8"},"source":["import torch\n","import torchvision\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","from torchvision.transforms import Resize, Compose, ToTensor, Normalize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lXzbFIoL07Kn"},"source":["## Setting Notebook"]},{"cell_type":"markdown","metadata":{"id":"dC1utPkx-WDa"},"source":["### Set seeds for repeatability"]},{"cell_type":"code","metadata":{"id":"b4wxatd1-Z0P","executionInfo":{"status":"ok","timestamp":1601570154916,"user_tz":-120,"elapsed":2122,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"cac80347-b1b1-457c-b957-285882451fcf","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.6.0+cu101\n","Torchvision Version:  0.7.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bMjJBG5a-eIi"},"source":["torch.manual_seed(0)\n","np.random.seed(0)\n","random.seed(0)\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","feature_extract = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5K2_rjcLQ7r5"},"source":["### Custom Argparser"]},{"cell_type":"code","metadata":{"id":"wybP1xvhQ9rw","executionInfo":{"status":"ok","timestamp":1601570154918,"user_tz":-120,"elapsed":2099,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"ce5c4efd-a503-4cd8-cc9e-f1abae923b64","colab":{"base_uri":"https://localhost:8080/"}},"source":["parser = argparse.ArgumentParser(description='PyTorch SirenNet Training')\n","\"\"\"parser.add_argument('data', metavar='DIR',\n","                    help='path to dataset')\"\"\"\n","parser.add_argument('--epochs', default=90, type=int, metavar='N',\n","                    help='number of total epochs to run')\n","parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n","                    help='manual epoch number (useful on restarts)')\n","parser.add_argument('-b', '--batch-size', default=256, type=int,\n","                    metavar='N',\n","                    help='mini-batch size (default: 256), this is the total '\n","                         'batch size of all GPUs on the current node when '\n","                         'using Data Parallel or Distributed Data Parallel')\n","parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n","                    metavar='LR', help='initial learning rate', dest='lr')\n","parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n","                    help='momentum')\n","parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n","                    metavar='W', help='weight decay (default: 1e-4)',\n","                    dest='weight_decay')\n","parser.add_argument('-p', '--print-freq', default=10, type=int,\n","                    metavar='N', help='print frequency (default: 10)')\n","parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n","                    help='evaluate model on validation set')\n","parser.add_argument('--seed', default=None, type=int,\n","                    help='seed for initializing training. ')\n","parser.add_argument('--gpu', default=None, type=int,\n","                    help='GPU id to use.')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--gpu'], dest='gpu', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help='GPU id to use.', metavar=None)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Gb19GFz--hg1"},"source":["### Set device - either gpu:0 or cpu"]},{"cell_type":"code","metadata":{"id":"e_PE_VJ7-hHT","executionInfo":{"status":"ok","timestamp":1601570154920,"user_tz":-120,"elapsed":2090,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"278a4d6f-1f58-4071-95a1-b8f26ec11c13","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Detect if we have a GPU available\n","# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = (torch.device('cuda:0') if torch.cuda.is_available()\n","    else torch.device('gpu'))\n","print(f\"Training on device {device}.\")\n","print(f\"# cuda device: {torch.cuda.device_count()}\")\n","print(f\"Id current device: {torch.cuda.current_device()}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training on device cuda:0.\n","# cuda device: 1\n","Id current device: 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"73gHw67dQB0v"},"source":["## Siren Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"_mhTSW7gQGhQ"},"source":["# Siren's architecture Hyper-params\n","in_features = 2\n","out_features = 1\n","sidelength = 256\n","hidden_layers = 3\n","\n","# Training phase's Hyper-params\n","learning_rate = 1e-4\n","\n","# Displaying Images\n","num_trials = 5\n","steps_til_summary = 10\n","\n","image_path = \"/content/drive/My Drive/Siren Deep Learning Analyses/testsets/BSD68/test001.png\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_3snv48WKEx"},"source":["root_analisys = \"/content/drive/My Drive/Siren Deep Learning Analyses/\"\n","\n","results_path = \"results\"\n","dataset_path = \"BSD68\"\n","images_path = \n","\n","csv_results_path = os.path.join(root_analisys, \"results\", dataset_path, \"csv\")\n","csv_results_path = os.path.join(root_analisys, \"results\", dataset_path, \"csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ef9EGPkzA_IG"},"source":["## Functions"]},{"cell_type":"markdown","metadata":{"id":"l6P06pMexFoK"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"G_8R0BaCy5vf"},"source":["def show_hist_kde_train_loss(bins = 10):\n","    samples = df['train_loss'].values\n","    n = len(samples)\n","    \n","    h, e = np.histogram(samples, bins=bins, density=True)\n","    x = np.linspace(e.min(), e.max())\n","\n","    # plot the histogram\n","    plt.figure(figsize=(8,6))\n","    plt.bar(e[:-1], h, width=np.diff(e), ec='k', align='edge', label='histogram')\n","\n","    # plot the real KDE\n","    kde = sts.gaussian_kde(samples)\n","    plt.plot(x, kde.pdf(x), c='C1', lw=8, label='KDE')\n","\n","    # resample the histogram and find the KDE.\n","    resamples = np.random.choice((e[:-1] + e[1:])/2, size=n*5, p=h/h.sum())\n","    rkde = sts.gaussian_kde(resamples)\n","\n","    # plot the KDE\n","    plt.plot(x, rkde.pdf(x), '--', c='C3', lw=4, label='resampled KDE')\n","    plt.title('Train Loss: n = %d samples' % n)\n","    plt.legend()\n","    plt.show()\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDnbi3B2xDw3"},"source":["def printImageAttributes(imageObject, imagePath):\n","\n","    # Retrieve the attributes of the image\n","\n","    fileFormat      = imageObject.format        # Format of the image\n","    imageMode       = imageObject.mode          # Mode of the image\n","    imageSize       = imageObject.size          # Size of the image - tupe of (width, height)\n","    colorPalette    = imageObject.palette       # Palette used in the image\n","\n","    # Print the attributes of the image\n","    print(\"Attributes of image:%s\"%imagePath)\n","\n","    print(\"The file format of the image is:%s\"%fileFormat)\n","\n","    print(\"The mode of the image is:%s\"%imageMode)\n","\n","    print(\"The size of the image is:width %d pixels,height %d pixels\"%imageSize)\n","\n","    print(\"Color palette used in image:%s\"%colorPalette)\n","\n","    print(\"Keys from image.info dictionary:%s\")\n","\n","    for key, value in imageObject.info.items() :\n","        print(key)\n","        print(value)\n","        pass\n","    pass\n","\n","def get_cameraman_tensor(sidelength):\n","    # print(type(skimage.data.camera()))\n","    img = Image.fromarray(skimage.data.camera())\n","    \n","    # print(type(img))\n","    printImageAttributes(img, \"cameraman\")\n","\n","    transform = Compose([\n","        Resize(sidelength),\n","        ToTensor(),\n","        Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n","    ])\n","    img = transform(img)\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AxqxasEBCN6"},"source":["def get_mgrid(sidelen, dim=2):\n","    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n","    sidelen: int\n","    dim: int'''\n","    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n","    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n","    mgrid = mgrid.reshape(-1, dim)\n","    return mgrid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Xl9dh8ln6rP"},"source":["def get_model(in_features, out_features, hidden_features, hidden_layers):\n","    img_siren = Siren(\n","        in_features = in_features,\n","        out_features = out_features,\n","        hidden_features = hidden_features, \n","        hidden_layers = hidden_layers,\n","        outermost_linear=True)\n","    img_siren.cuda()\n","    return img_siren"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUbFJigcoIub"},"source":["def get_information_model(model):\n","    model = model.to(device)\n","\n","    numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n","    print(model)\n","    print(sum(numel_list), numel_list)\n","\n","    params_to_update = model.parameters()\n","    print(\"Params to learn:\")\n","    if feature_extract:\n","        params_to_update = []\n","        for name,param in model.named_parameters():\n","            if param.requires_grad == True:\n","                params_to_update.append(param)\n","                print(\"\\t\",name)\n","    else:\n","        for name,param in model.named_parameters():\n","            if param.requires_grad == True:\n","                print(\"\\t\",name)\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qmMEVQfdnOK0"},"source":["### Trainining Loop: Show Image Estimate Progression"]},{"cell_type":"code","metadata":{"id":"IfO1Yw9sFIjU"},"source":["def show_images(model_output, coords):\n","    img_grad = gradient(model_output, coords)\n","    img_laplacian = laplace(model_output, coords)\n","\n","    fig, axes = plt.subplots(1,3, figsize=(18,6))\n","    axes[0].imshow(model_output.cpu().view(sidelength, sidelength).detach().numpy())\n","    axes[1].imshow(img_grad.norm(dim=-1).cpu().view(sidelength, sidelength).detach().numpy())\n","    axes[2].imshow(img_laplacian.cpu().view(sidelength, sidelength).detach().numpy())\n","    plt.show()\n","    pass\n","\n","def basic_traininig_loop(optimizer, criterion, model, model_input, ground_truth, total_steps, steps_til_summary = 10):\n","    \n","    train_acc_history = [] # val_acc_history = []\n","    train_loss_history = [] # val_loss_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    model.train()  # Set model to training mode\n","\n","    for step in range(total_steps):\n","        # print('Epoch {}/{}'.format(step, total_steps - 1))\n","        # print('-' * 10)\n","\n","        model_output, coords = model(model_input)\n","        loss = criterion(model_output, ground_truth)  \n","        # loss = ((model_output - ground_truth)**2).mean()\n","    \n","        if not step % steps_til_summary:\n","            print(\"Step %d, Total loss %0.6f\" % (step, loss))\n","            show_images(model_output, coords)\n","            pass\n","        \n","        train_loss_history.append(loss)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        pass\n","    \n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    # create history as python dictionary\n","    keys_history_list = \"train_loss\".split(\",\")\n","    values_history_list = [train_loss_history]\n","\n","    history = dict(zip(keys_history_list, values_history_list))\n","\n","    return model, history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-un2QhR5nXmn"},"source":["### Plane Training Loop"]},{"cell_type":"code","metadata":{"id":"eFniZVE-SaKm"},"source":["def plane_traininig_loop(optimizer, criterion, model, model_input, ground_truth, total_steps, verbose = 0):\n","    \n","    train_acc_history = [] # val_acc_history = []\n","    train_loss_history = [] # val_loss_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    model.train()  # Set model to training mode\n","\n","    phase = 'train'\n","    for step in range(total_steps):\n","        if verbose == 1:\n","            print('Epoch {}/{}'.format(step, total_steps - 1))\n","            print('-' * 10)\n","\n","        model_output, coords = model(model_input)    \n","        loss = criterion(model_output, ground_truth)\n","        # loss = ((model_output - ground_truth)**2).mean()\n","        \n","        if verbose == 1:\n","            print('{} Loss: {:.4f}'.format(phase, loss))\n","        train_loss_history.append(loss)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        pass\n","    \n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    # create history as python dictionary\n","    keys_history_list = \"train_loss\".split(\",\")\n","    values_history_list = [train_loss_history]\n","\n","    history = dict(zip(keys_history_list, values_history_list))\n","\n","    return model, history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sLX5vsuDX7f"},"source":["### Training Loop: Run through a whole set of images"]},{"cell_type":"code","metadata":{"id":"9TUkmxc_DYmN"},"source":["def a_training_loop(images, first_image_pos, last_image_pos, sidelength = 128, lr = 1e-4, total_steps = 500, verbose = 0):\n","    for _, image_path in enumerate(images[first_image_pos:last_image_pos]):\n","\n","        # Create a DataLoader Instance for current image to be fitted\n","        # -------------------------------------------------------------------- #\n","        image = ImageFitting(\n","            sidelength,\n","            image_path = image_path)\n","        dataloader = DataLoader(image,\n","            batch_size=1,\n","            pin_memory=True,\n","            num_workers=0)\n","\n","        img_siren = get_model(\n","            in_features = in_features,\n","            out_features = out_features,\n","            hidden_features = sidelength,\n","            hidden_layers = hidden_layers)\n","\n","        # Establish Number of steps, criterion, and loss function\n","        # -------------------------------------------------------------------- #\n","        # total_steps = 500 \n","        # Since the whole image is our dataset, this just means 500 gradient descent steps.\n","\n","        criterion = nn.MSELoss()\n","        optim = torch.optim.Adam(lr=lr, params=img_siren.parameters())\n","\n","        model_input, ground_truth = next(iter(dataloader))\n","        model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n","\n","        # Proceed with training\n","        # -------------------------------------------------------------------- #\n","        start = time.time()\n","        # model, history = basic_traininig_loop(\n","        model, history = plane_traininig_loop(\n","            optimizer = optim,\n","            criterion = criterion,\n","            model = img_siren,\n","            model_input = model_input,\n","            ground_truth = ground_truth,\n","            # total_steps = total_steps, steps_til_summary = steps_til_summary)\n","            total_steps = total_steps)\n","        stop = time.time()\n","\n","        # times = (stop - start) * 1000\n","        times = (stop - start)\n","        if verbose == 1:\n","            print('-' * 40)\n","            # print('Run time takes %d miliseconds' % times) # print('Run time takes %.3f seconds' % times)\n","            print('Training complete in {:.0f}m {:.0f}s {:.0f}ms'\n","                .format(times // 60, times % 60, (times - int((times % 60))) * 1000))\n","\n","        # Save results for current trained image\n","        # -------------------------------------------------------------------- #\n","        image_name = os.path.basename(image_path)\n","        last_loss_train = history['train_loss'][-1]\n","        a_record = [image_name, last_loss_train.item(), times]\n","\n","        records_list.append(a_record)\n","        pass\n","    return model, history, records_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UgE6UqEN-zhD"},"source":["## Siren Classes\n","\n","Now, we code up the sine layer, which will be the basic building block of SIREN. This is a much more concise implementation than the one in the main code, as here, we aren't concerned with the baseline comparisons."]},{"cell_type":"code","metadata":{"id":"RMSzO1kWxST0"},"source":["class ImageFitting(Dataset):\n","    def __init__(self, sidelength, image_path = None):\n","        super().__init__()\n","        if image_path is None:\n","            img = get_cameraman_tensor(sidelength)\n","        else:\n","            imag = Image.open(image_path)\n","            imag = imag.resize((sidelength, sidelength))\n","            # printImageAttributes(imag, image_path)\n","            # imag = np.asarray(imag)\n","            transform = Compose([\n","                Resize(sidelength),\n","                ToTensor(),\n","                Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n","            ])\n","            img = transform(imag)\n","            pass\n","        \n","        # print(type(img))\n","        # print(img.size())\n","\n","        self.pixels = img.permute(1, 2, 0).view(-1, 1)\n","        self.coords = get_mgrid(sidelength, 2)\n","        pass\n","\n","    def __len__(self):\n","        return 1\n","\n","    def __getitem__(self, idx):    \n","        if idx > 0: raise IndexError\n","            \n","        return self.coords, self.pixels\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyOSBFpp-_f6"},"source":["class SineLayer(nn.Module):\n","    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n","    \n","    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the \n","    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a \n","    # hyperparameter.\n","    \n","    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of \n","    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n","    \n","    def __init__(self, in_features, out_features, bias=True,\n","                 is_first=False, omega_0=30):\n","        super().__init__()\n","        self.omega_0 = omega_0\n","        self.is_first = is_first\n","        \n","        self.in_features = in_features\n","        self.linear = nn.Linear(in_features, out_features, bias=bias)\n","        \n","        self.init_weights()\n","    \n","    def init_weights(self):\n","        with torch.no_grad():\n","            if self.is_first:\n","                self.linear.weight.uniform_(-1 / self.in_features, \n","                                             1 / self.in_features)      \n","            else:\n","                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n","                                             np.sqrt(6 / self.in_features) / self.omega_0)\n","        \n","    def forward(self, input):\n","        return torch.sin(self.omega_0 * self.linear(input))\n","    \n","    def forward_with_intermediate(self, input): \n","        # For visualization of activation distributions\n","        intermediate = self.omega_0 * self.linear(input)\n","        return torch.sin(intermediate), intermediate\n","    \n","    \n","class Siren(nn.Module):\n","    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n","                 first_omega_0=30, hidden_omega_0=30.):\n","        super().__init__()\n","        \n","        self.net = []\n","        self.net.append(SineLayer(in_features, hidden_features, \n","                                  is_first=True, omega_0=first_omega_0))\n","\n","        for i in range(hidden_layers):\n","            self.net.append(SineLayer(hidden_features, hidden_features, \n","                                      is_first=False, omega_0=hidden_omega_0))\n","\n","        if outermost_linear:\n","            final_linear = nn.Linear(hidden_features, out_features)\n","            \n","            with torch.no_grad():\n","                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n","                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n","                \n","            self.net.append(final_linear)\n","        else:\n","            self.net.append(SineLayer(hidden_features, out_features, \n","                                      is_first=False, omega_0=hidden_omega_0))\n","        \n","        self.net = nn.Sequential(*self.net)\n","    \n","    def forward(self, coords):\n","        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n","        output = self.net(coords)\n","        return output, coords        \n","\n","    def forward_with_activations(self, coords, retain_grad=False):\n","        '''Returns not only model output, but also intermediate activations.\n","        Only used for visualizing activations later!'''\n","        activations = OrderedDict()\n","\n","        activation_count = 0\n","        x = coords.clone().detach().requires_grad_(True)\n","        activations['input'] = x\n","        for i, layer in enumerate(self.net):\n","            if isinstance(layer, SineLayer):\n","                x, intermed = layer.forward_with_intermediate(x)\n","                \n","                if retain_grad:\n","                    x.retain_grad()\n","                    intermed.retain_grad()\n","                    \n","                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n","                activation_count += 1\n","            else: \n","                x = layer(x)\n","                \n","                if retain_grad:\n","                    x.retain_grad()\n","                    \n","            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n","            activation_count += 1\n","\n","        return activations"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTJvYEF__NcU"},"source":["And finally, differential operators that allow us to leverage autograd to compute gradients, the laplacian, etc."]},{"cell_type":"code","metadata":{"id":"LnGeVmLq_M62"},"source":["def laplace(y, x):\n","    grad = gradient(y, x)\n","    return divergence(grad, x)\n","\n","\n","def divergence(y, x):\n","    div = 0.\n","    for i in range(y.shape[-1]):\n","        div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n","    return div\n","\n","\n","def gradient(y, x, grad_outputs=None):\n","    if grad_outputs is None:\n","        grad_outputs = torch.ones_like(y)\n","    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n","    return grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lptl7BN5-1qd"},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{"id":"K-wC41Ve_yeW"},"source":["### Fitting an image\n","\n","First, let's simply fit that image!\n","\n","We seek to parameterize a greyscale image $f(x)$ with pixel coordinates $x$ with a SIREN $\\Phi(x)$.\n","\n","That is we seek the function $\\Phi$ such that: $\\mathcal{L}=\\int_{\\Omega} \\lVert \\Phi(\\mathbf{x}) - f(\\mathbf{x}) \\rVert\\mathrm{d}\\mathbf{x}$ is minimized, in which $\\Omega$ is the domain of the image.\n","\n","We write a little datast that does nothing except calculating per-pixel coordinates:"]},{"cell_type":"markdown","metadata":{"id":"kxxukLZsAam-"},"source":["Let's instantiate the dataset and our Siren. As pixel coordinates are 2D, the siren has 2 input features, and since the image is grayscale, it has one output channel."]},{"cell_type":"code","metadata":{"id":"LF4ktPGzpGwT"},"source":["images = list()\n","bsd68_path = \"/content/drive/My Drive/Siren Deep Learning Analyses/testsets/BSD68\"\n","for (dirpath, dirnames, filenames) in os.walk(bsd68_path):\n","    full_file_path_list = list()\n","    for a_file in sorted(filenames):\n","        full_file_path_list.append(os.path.join(dirpath, a_file))\n","        pass\n","    images.extend(full_file_path_list)\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NW7S_vjDE0O3"},"source":["records_list = list()\n","first_image_pos, last_image_pos = 0, len(images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utsIR5JIF0A4","executionInfo":{"status":"error","timestamp":1601572383375,"user_tz":-120,"elapsed":2230370,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"a389218f-3f7a-41c4-8c7a-b5fd71e1f5fb","colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["df, tmp_df = None, None\n","for ii, a_trial in enumerate(range(num_trials)):\n","    start = time.time()\n","\n","    model, history, records = a_training_loop(images, first_image_pos, last_image_pos, sidelength = sidelength, lr = 1e-4, total_steps = 500)\n","\n","    stop = time.time()\n","    times = (stop - start)\n","    print('-' * 40)\n","    print('[Trial no.{:.0f}] [Outer Loop] Training complete in {:.0f}m {:.0f}s {:.0f}ms'\n","        .format(a_trial, times // 60, times % 60, (times - int((times % 60))) * 1000))\n","    \n","    \n","    if tmp_df is None:\n","        columns = \"image,train_loss,time\".split(\",\")\n","        df = pd.DataFrame(data = records,  columns = columns, index = [f\"trial no.{ii}\"] * len(records))\n","        df_show_curr_stats = df\n","    else:\n","        columns = \"image,train_loss,time\".split(\",\")\n","        tmp_df = pd.DataFrame(data = records,  columns = columns, index = [f\"trial no.{ii}\"] * len(records))\n","        df_show_curr_stats = tmp_df\n","        df = pd.concat([df, tmp_df])\n","    \n","    df_show_curr_stats[['train_loss']].describe().T\n","    pass"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-4b9a5066de42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_image_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_image_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msidelength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msidelength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-a20bc98c06b9>\u001b[0m in \u001b[0;36ma_training_loop\u001b[0;34m(images, first_image_pos, last_image_pos, sidelength, lr, total_steps, verbose)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# total_steps = total_steps, steps_til_summary = steps_til_summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             total_steps = total_steps)\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-08f37352a391>\u001b[0m in \u001b[0;36mplane_traininig_loop\u001b[0;34m(optimizer, criterion, model, model_input, ground_truth, total_steps, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbest_model_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"vsqdu8fXrm5B"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1g2GNXuhuXUZ"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5snm5SMLyhN"},"source":["tmp_df = df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ok1OufjKJ0p6"},"source":["num_images = last_image_pos - first_image_pos\n","sol = []\n","for ii in range(num_images):\n","    indeces = np.arange(0, num_trials) * num_images + ii\n","    # pprint(indeces)\n","    ii_values = df[['train_loss']].iloc[indeces].values\n","    ii_mean = np.mean(ii_values)\n","    sol.append([ii_mean])\n","    pass\n","\n","columns = \"train_loss\".split(\",\")\n","df = pd.DataFrame(data = sol,  columns = columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFuQ4Ppnvb-y"},"source":["boxplot = df.boxplot(column = ['train_loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1HCIsATyQA3"},"source":["# hist = df.hist(column = ['train_loss'], bins = 10)\n","# ax = df[['train_loss']].plot.kde()\n","show_hist_kde_train_loss(bins = 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8t7yjXAbtLlF"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"Q5HPv4MQTLuZ"},"source":["### Compression Algorithms:\n","\n","- [zlib](http://www.zlib.net/)"]},{"cell_type":"markdown","metadata":{"id":"3XmvgkwENDdc"},"source":["### Tutorials:\n","- [Data To Fish](https://datatofish.com/)"]},{"cell_type":"markdown","metadata":{"id":"egnkME2pw0Jt"},"source":["### Statistics Topics:\n","\n","- [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation)\n","  - [Stack Overflow Example](https://stackoverflow.com/questions/53823349/how-can-you-create-a-kde-from-histogram-values-only)"]},{"cell_type":"markdown","metadata":{"id":"KLPW9SIbuuj8"},"source":["### Papers\n","\n","- [Siren](https://arxiv.org/abs/2006.09661)"]},{"cell_type":"markdown","metadata":{"id":"xfdxbBpMtOm0"},"source":["### Libraries\n","\n","- [Standard Python Library](https://docs.python.org/3/library/)\n","- [Pillow Lib](https://pillow.readthedocs.io/en/stable/index.html)\n","- [PyTorch Lib](https://pytorch.org/)\n","- [Pandas Lib](https://pandas.pydata.org/pandas-docs/stable/index.html)"]},{"cell_type":"markdown","metadata":{"id":"W0D9eAUmu2jH"},"source":["## Citations\n","\n","@misc{sitzmann2020implicit,\n","    title={Implicit Neural Representations with Periodic Activation Functions},\n","    author={Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},\n","    year={2020},\n","    eprint={2006.09661},\n","    archivePrefix={arXiv},\n","    primaryClass={cs.CV}\n","}"]}]}