{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"siren-show-accuracy-graphics.ipynb","provenance":[{"file_id":"1LR7Xes_nDrmWLdhv4DbFLiR6tTIGAjVV","timestamp":1601238917705}],"collapsed_sections":["l6SgtyvTPHHI","LyrqG0fSPN2x","FBBgv3Y7PSK5","NDd_cdqyPaGf"],"authorship_tag":"ABX9TyOyzs9XWXB+PX3tnMCG6c+K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"l6SgtyvTPHHI"},"source":["## Installing Third Party Libraries"]},{"cell_type":"code","metadata":{"id":"JOD18qmJ587s","executionInfo":{"status":"ok","timestamp":1601238801375,"user_tz":-120,"elapsed":3859,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"502fea6f-e3a1-44db-adaa-ab6b3d3e23de","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install visdom"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: visdom in /usr/local/lib/python3.6/dist-packages (0.1.8.9)\n","Requirement already satisfied: torchfile in /usr/local/lib/python3.6/dist-packages (from visdom) (0.1.0)\n","Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from visdom) (1.26)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom) (1.15.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom) (2.23.0)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.6/dist-packages (from visdom) (0.57.0)\n","Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from visdom) (1.18.5)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom) (19.0.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom) (7.0.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom) (5.1.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->visdom) (2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LyrqG0fSPN2x"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"aFT4cPbl6YWK"},"source":["from __future__ import print_function\n","from __future__ import division\n","\n","import visdom\n","import torch\n","import torchvision\n","\n","import datetime\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dWhkeRMCpH0","executionInfo":{"status":"ok","timestamp":1601238801378,"user_tz":-120,"elapsed":3796,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"f815729e-2354-4cbf-9490-5850bbd14597","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.6.0+cu101\n","Torchvision Version:  0.7.0+cu101\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FBBgv3Y7PSK5"},"source":["## Set Seeds & Device "]},{"cell_type":"code","metadata":{"id":"I3QiOsIp-SDg"},"source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","feature_extract = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqwSem2rAL9G","executionInfo":{"status":"ok","timestamp":1601238801379,"user_tz":-120,"elapsed":3719,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"2a9a21cd-714c-479d-f8b4-a08af2bb17d1","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Detect if we have a GPU available\n","# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = (torch.device('cuda:0') if torch.cuda.is_available()\n","    else torch.device('gpu'))\n","print(f\"Training on device {device}.\")\n","print(f\"# cuda device: {torch.cuda.device_count()}\")\n","print(f\"Id current device: {torch.cuda.current_device()}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training on device cuda:0.\n","# cuda device: 1\n","Id current device: 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aYCe4NccB6AN","executionInfo":{"status":"ok","timestamp":1601238801380,"user_tz":-120,"elapsed":3693,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"b17d4e28-2dd3-4b8d-dc4d-3d44f4da2291","colab":{"base_uri":"https://localhost:8080/"}},"source":["'''vis = visdom.Visdom()\n","vis.text('Hello, world!')\n","vis.image(np.ones((3, 10, 10)))'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"vis = visdom.Visdom()\\nvis.text('Hello, world!')\\nvis.image(np.ones((3, 10, 10)))\""]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"rOMKlRgoPWus"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"MsEDxo5l8_oP"},"source":["def load_train_val_sets(batch_size = 64, data_path = '/content/cifar-10-batches-py'):\n","\n","    class_objects = \"airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck\".split(\",\")\n","\n","    class_indeces = range(0, 10)\n","    item_pairs = zip(class_indeces, class_objects)\n","    class_names = dict(item_pairs)\n","\n","    class_indeces = range(0, 10)\n","    item_pairs_reverse = zip(class_objects, class_indeces)\n","    class_names_reverse = dict(item_pairs_reverse)\n","\n","    tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=True,\n","    transform=transforms.ToTensor())\n","\n","    imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n","    mean_by_channels = imgs.view(3, -1).mean(dim=1)\n","    std_by_channels = imgs.view(3, -1).std(dim=1)\n","\n","    transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n","        transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=mean_by_channels, std=std_by_channels)\n","            ]\n","        )\n","    )\n","\n","    transformed_cifar10_val = datasets.CIFAR10(data_path, train=False, download=True,\n","        transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=mean_by_channels, std=std_by_channels)\n","            ]\n","        )\n","    )\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        transformed_cifar10, batch_size = batch_size, shuffle = True\n","    )\n","\n","    val_loader = torch.utils.data.DataLoader(\n","        transformed_cifar10_val, batch_size = batch_size, shuffle = False\n","    )\n","    return train_loader, val_loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxSE-8e-8qV8"},"source":["def convnet_validate(model, train_loader, val_loader):\n","    \"\"\"\n","    Validate model's performance calculating Accuracy scores for both training\n","    set and validation set, where input model's is made of at least one convolution layer.\n","\n","    Parameters:\n","    -----------\n","    :model: either nn.Sequential or subclass from nn.Module\n","    :train_loader: either nn.DataLoader or nn.DataSet instances collecting data examples employed for training the model\n","    :val_loader: either nn.DataLoader or nn.DataSet instances collecting data examples constituting validation set\n","\n","    Returns:\n","    --------\n","    :result_str: str Python object for later dispalying containing as a message \n","        the Accuracy scores calculated for both train and val set passed independently to model instance \n","    \"\"\"\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    result_str = \"\"\n","\n","    for name, loader in [('train', train_loader), ('val', val_loader)]:\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for imgs, labels in loader:\n","                imgs = imgs.to(device=device)\n","                labels = labels.to(device=device)\n","\n","                outputs = model(imgs)\n","                _, predicted = torch.max(outputs, dim = 1)\n","                \n","                total += labels.shape[0]\n","                correct += int((predicted == labels).sum())\n","                pass\n","        result_str += \"Accuracy {}: {:.5f} \".format(name, correct / total)\n","    \n","    return result_str"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wcrhEmsY8t1m"},"source":["def batched_convnet_training_loop(model, loss_fn, optimizer, train_loader, val_loader, n_epochs = 10):\n","    \"\"\"\n","    More Advanced training loop for brief tests exploiting Batch Strategy to let training be more stable and smooth, \n","    where model's arch is represented by a ConvNet.\n","    \n","    Parameters:\n","    -----------\n","    :model: either nn.Sequential or subclass from nn.Module\n","    :loss_fn: loss function pytorch's instance, expressing a particular loss function shape to be minimized by means of a particular optimization strategy\n","    :optimizer: optimizer pytorch's instance representing the selected optimization strategy to be followed to fit the model's arch to the train data\n","    :train_loader: either nn.DataLoader or nn.DataSet instances collecting data examples employed for training the model\n","    :val_loader: either nn.DataLoader or nn.DataSet instances collecting data examples constituting validation set\n","\n","    Returns:\n","    --------\n","    None \n","    \"\"\"\n","\n","    # Initialize the visualization environment\n","    # vis = Visualizations()\n","\n","    # Training loop\n","    loss_values = []\n","    for epoch in range(1, n_epochs + 1):\n","\n","        # switch to train mode\n","        model.train()\n","        \n","        for step, (imgs, labels) in enumerate(train_loader):\n","            imgs = imgs.to(device=device)\n","            labels = labels.to(device=device)\n","            outputs = model(imgs)\n","            loss = loss_fn(outputs, labels)\n","            loss_values.append(loss.item())\n","        \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Visualization data\n","            \"\"\"if step % 10 == 0:\n","                vis.plot_loss(np.mean(loss_values), step)\n","                loss_values.clear()\"\"\"\n","            pass\n","\n","        result_validate_str = convnet_validate(model, train_loader, val_loader)\n","        \n","        epoch_msg = \"%s Epoch: %d, Trainin Loss: %.5f %s\" \\\n","              % (str(datetime.datetime.now()),\n","                 epoch, float(loss),\n","                 result_validate_str.strip()\n","                 )\n","        print(epoch_msg)\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fmFcghKDL9i"},"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    train_acc_history = []\n","\n","    val_loss_history = []\n","    train_loss_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","                val_loss_history.append(epoch_loss)\n","            else:\n","                train_acc_history.append(epoch_acc)\n","                train_loss_history.append(epoch_loss)\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    history = dict(\n","        zip(\"train_acc,val_acc,train_loss,val_loss\".split(\",\"),\n","            [train_acc_history, val_acc_history, train_loss_history, val_loss_history])\n","    )\n","    return model, history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDd_cdqyPaGf"},"source":["## Classes"]},{"cell_type":"code","metadata":{"id":"Eh7IbVlq8gGW"},"source":["class Visualizations:\n","    def __init__(self, env_name=None):\n","        if env_name is None:\n","            env_name = str(datetime.datetime.now().strftime(\"%d-%m %Hh%M\"))\n","        self.env_name = env_name\n","        self.vis = visdom.Visdom(env=self.env_name)\n","        self.loss_win = None\n","\n","    def plot_loss(self, loss, step):\n","        self.loss_win = self.vis.line(\n","            [loss],\n","            [step],\n","            win=self.loss_win,\n","            update='append' if self.loss_win else None,\n","            opts=dict(\n","                xlabel='Step',\n","                ylabel='Loss',\n","                title='Loss (mean per 10 steps)',\n","            )\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R13aeEYl7NAL"},"source":["class Net(nn.Module):\n","    def __init__(self, num_classes = 2):\n","        super().__init__()\n","        \n","        # First Conv + Act + Pool full block\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, padding = 1)\n","        self.act1 = nn.Tanh()\n","        self.pool1 = nn.MaxPool2d(2)\n","        \n","        # Second Conv + Act + Pool full block\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size = 3, padding = 1)\n","        self.act2 = nn.Tanh()\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        # Fully Connected layers on top NN Arch.\n","        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n","        self.act3 = nn.Tanh()\n","        self.fc2 = nn.Linear(32, num_classes)\n","        pass\n","\n","    def forward(self, x):\n","        out = self.pool1(self.act1(self.conv1(x)))\n","        out = self.pool2(self.act2(self.conv2(out)))\n","        out = out.view(-1, 8 * 8 * 8)\n","        out = self.act3(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kw0BVcT2PdOo"},"source":["## Load Data & Buld Model"]},{"cell_type":"code","metadata":{"id":"AOWUeojp9HXj","executionInfo":{"status":"ok","timestamp":1601238801384,"user_tz":-120,"elapsed":3625,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"7a64e8bf-b2dd-4aee-f553-fac562543e28","colab":{"base_uri":"https://localhost:8080/"}},"source":["# build a model\n","model = Siren(in_features=2, out_features=3, hidden_features=256, \n","                  hidden_layers=3, outermost_linear=True)\n","numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n","print(model)\n","print(sum(numel_list), numel_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (act1): Tanh()\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (act2): Tanh()\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=512, out_features=32, bias=True)\n","  (act3): Tanh()\n","  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",")\n","18354 [432, 16, 1152, 8, 16384, 32, 320, 10]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"giFou45a9b0_"},"source":["learning_rate = 1e-3\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n","\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ykbEbzMj9TI7","executionInfo":{"status":"ok","timestamp":1601238812864,"user_tz":-120,"elapsed":15086,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"42a0db53-f7bf-435f-c1ac-0263bd87d201","colab":{"base_uri":"https://localhost:8080/"}},"source":["train_loader, val_loader = load_train_val_sets()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l2xk-TmuE3Em","executionInfo":{"status":"ok","timestamp":1601238815747,"user_tz":-120,"elapsed":17960,"user":{"displayName":"francesco chiarlo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDAIKCUiswKi29zv4U3Oejsg31qDN02lqmfaV9=s64","userId":"17164257904618749375"}},"outputId":"4f75e674-3749-4324-94d1-27cb3ec06e65","colab":{"base_uri":"https://localhost:8080/"}},"source":["model = model.to(device)\n","params_to_update = model.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Params to learn:\n","\t conv1.weight\n","\t conv1.bias\n","\t conv2.weight\n","\t conv2.bias\n","\t fc1.weight\n","\t fc1.bias\n","\t fc2.weight\n","\t fc2.bias\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TXT7LeU0PdNK"},"source":["## Run Training Phase"]},{"cell_type":"code","metadata":{"id":"q7ZBfPFF8jgg","outputId":"ee3e6ec8-18ab-44ba-e159-87982b8c84cf","colab":{"base_uri":"https://localhost:8080/","height":731}},"source":["# batched_convnet_training_loop(model.to(device), criterion, optimizer, train_loader, val_loader, n_epochs = 10)\n","num_epochs = 100\n","dataloaders = dict(zip(['train', 'val'], [train_loader, val_loader]))\n","model, history = train_model(model, dataloaders, criterion, optimizer, num_epochs=num_epochs, is_inception=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0/99\n","----------\n","train Loss: 2.2838 Acc: 0.1564\n","val Loss: 2.2523 Acc: 0.1889\n","\n","Epoch 1/99\n","----------\n","train Loss: 2.2089 Acc: 0.2177\n","val Loss: 2.1597 Acc: 0.2415\n","\n","Epoch 2/99\n","----------\n","train Loss: 2.1204 Acc: 0.2517\n","val Loss: 2.0777 Acc: 0.2681\n","\n","Epoch 3/99\n","----------\n","train Loss: 2.0502 Acc: 0.2784\n","val Loss: 2.0175 Acc: 0.2966\n","\n","Epoch 4/99\n","----------\n","train Loss: 1.9987 Acc: 0.3012\n","val Loss: 1.9738 Acc: 0.3157\n","\n","Epoch 5/99\n","----------\n","train Loss: 1.9604 Acc: 0.3174\n","val Loss: 1.9399 Acc: 0.3305\n","\n","Epoch 6/99\n","----------\n","train Loss: 1.9292 Acc: 0.3287\n","val Loss: 1.9107 Acc: 0.3440\n","\n","Epoch 7/99\n","----------\n","train Loss: 1.9012 Acc: 0.3399\n","val Loss: 1.8830 Acc: 0.3581\n","\n","Epoch 8/99\n","----------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ExwlsZJ3PnW0"},"source":["## Show graphics"]},{"cell_type":"code","metadata":{"id":"Offlfo5GEV5I"},"source":["# Plot the training curves of train / validation accuracy vs. number\n","#  of training epochs for the transfer learning method and\n","#  the model trained from scratch\n","train_acc_h = []\n","val_acc_h = []\n","\n","train_acc_h = [h.cpu().numpy() for h in history['train_acc']]\n","val_acc_h = [h.cpu().numpy() for h in history['val_acc']]\n","\n","plt.title(\"Train / Validation Accuracy vs. Number of Training Epochs\")\n","plt.xlabel(\"Training Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.plot(range(1,num_epochs+1), train_acc_h, label=\"train_acc\")\n","plt.plot(range(1,num_epochs+1), val_acc_h, label=\"val_acc\")\n","plt.ylim((0,1.))\n","plt.xticks(np.arange(1, num_epochs+1, 1.0))\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U12owi7FKBt_"},"source":["# Plot the training curves of train validation loss vs. number\n","#  of training epochs for the transfer learning method and\n","#  the model trained from scratch\n","train_loss_h = []\n","val_loss_h = []\n","\n","train_loss_h = [h for h in history['train_loss']]\n","val_loss_h = [h for h in history['val_loss']]\n","\n","plt.title(\"Train / Validation Loss vs. Number of Training Epochs\")\n","plt.xlabel(\"Training Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.plot(range(1,num_epochs+1), train_loss_h, label=\"train_loss\")\n","plt.plot(range(1,num_epochs+1), val_acc_h, label=\"val_loss\")\n","# plt.ylim((0,1.))\n","plt.xticks(np.arange(1, num_epochs+1, 1.0))\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-Fwr5MKAg9A"},"source":["## References\n","\n","- Pytorch Reference's Manual:\n"," - [torch.nn module](https://pytorch.org/docs/stable/nn.html)\n"," - [TensorBoard Support](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n"," - [Train Example](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code)\n","\n","- Initialization Topic (Papers):\n","  - [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by X. Glorot & Y.Bengio, which lead to default Pytorch's weights initialization knwon as *Xavier initialization* algorithm or scheme\n"," -  [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321) by Hongyi Zhang, Yann N. Dauphin, Tengyu Ma, whose works allows to *get rid off batch normalization layers* with a given particular NN Arch, to still be able to train a NN arch with meaningful and confident results or performance.\n","\n","- Activation Functions (Papers):\n","  - [Deep Learning using Rectified Linear Units (ReLU)](https://arxiv.org/pdf/1803.08375.pdf)\n","\n","- Datasets:\n","  - [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n","\n","- Regularization techniques (Papers):\n","  - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif](https://arxiv.org/abs/1502.03167)\n","  - [Dropout: A Simple Way to Prevent Neural Networks from\n","Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n","\n","- Archs Types (Papers):\n","  - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n","  - [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n","  - [Highway Networks\n","](https://arxiv.org/pdf/1505.00387.pdf)\n","  - [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n","\n","- Some Third Party useful Tutorials:\n"," - [Imagenet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L327)\n"," - [Writing a better code with pytorch and einops](https://arogozhnikov.github.io/einops/pytorch-examples.html)\n"," - [Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric - MEDIUM](https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8)\n"," - [Pytorch Example](https://pythonprogramming.net/analysis-visualization-deep-learning-neural-network-pytorch/)\n","\n","- Books\n","  - [List of books for improving Pytorch knowledge](https://bookauthority.org/books/best-pytorch-books)\n","\n","- GitHub Projetcs:\n","  - [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric)\n","  - [Minetorch](https://github.com/minetorch/minetorch)\n","  - [Pierogi](https://github.com/nalepae/pierogi/)\n","  - [Visdom](https://github.com/facebookresearch/visdom#vismatplot)"]}]}